{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import goodfire\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('justice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ID', 'name', 'href', 'docket', 'term', 'first_party',\n",
       "       'second_party', 'facts', 'facts_len', 'majority_vote', 'minority_vote',\n",
       "       'first_party_winner', 'decision_type', 'disposition', 'issue_area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision_type\n",
       "majority opinion                     2829\n",
       "per curiam                            267\n",
       "plurality opinion                     153\n",
       "equally divided                        17\n",
       "dismissal - rule 46                     9\n",
       "dismissal - other                       8\n",
       "dismissal - improvidently granted       6\n",
       "dismissal - moot                        5\n",
       "memorandum                              1\n",
       "opinion of the court                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['decision_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_area\n",
       "Criminal Procedure      859\n",
       "Civil Rights            568\n",
       "Economic Activity       542\n",
       "First Amendment         353\n",
       "Judicial Power          342\n",
       "Due Process             128\n",
       "Federalism              125\n",
       "Privacy                  70\n",
       "Unions                   60\n",
       "Federal Taxation         51\n",
       "Attorneys                37\n",
       "Miscellaneous            20\n",
       "Private Action            4\n",
       "Interstate Relations      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['issue_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions = df[(df[\"decision_type\"] == \"majority opinion\")&(df[\"facts_len\"] >= 2500)][[\"name\",\"term\",\"facts\",\"decision_type\",\"first_party\",\"second_party\",\"first_party_winner\",\"issue_area\",\"facts_len\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_area\n",
       "Civil Rights          11\n",
       "Economic Activity      9\n",
       "Criminal Procedure     7\n",
       "Judicial Power         6\n",
       "Federalism             2\n",
       "First Amendment        2\n",
       "Federal Taxation       1\n",
       "Unions                 1\n",
       "Miscellaneous          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_opinions[\"issue_area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = majority_opinions['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.str.replace('<p>','')\n",
    "facts = facts.str.replace('<p dir=\"ltr\">','')\n",
    "facts = facts.str.replace('</p>','')\n",
    "facts = facts.str.replace('/n','')\n",
    "facts = facts.str.replace('<em>','')\n",
    "facts = facts.str.replace('</em>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = majority_opinions['term'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No content found between the specified substrings.\n",
      "The text provided does not contain any explicit mention of a Supreme Court decision, so no cleaning is required based on the instructions given. However, if there were a Supreme Court decision mentioned after a colon, it would be removed unless it was made prior to 2011. Since there is no such instance in the text, the text remains unchanged. Here is the original text for reference:\n",
      "\n",
      "---\n",
      "\n",
      "Plaintiffs Stephen R. Chandler and Robert L. Pierce were the sole shareholders of Home Oil and Coal Company, Inc. In 1999, Pierce contemplated selling his share of the business and sought professional advice in an effort to minimize tax liability generated by the sale of his interest in Home Oil. Each of the taxpayers initiated short sales of United States Treasury Bonds for $7,472,405. They then transferred the proceeds from that sale to Home Concrete as capital contributions. Home Concrete then closed the short sales by purchasing and returning essentially identical Treasury Bonds on the open market for $7,359,043. This transaction created \"outside basis,\" or how much the partner's investment was worth according to tax rules, equal to the amount of the proceeds the taxpayers contributed.\n",
      "\n",
      "Home Oil then transferred its assets to Home Concrete as a capital contribution. The taxpayers (except Home Oil) then transferred percentages of their partnership interests in Home Concrete to Home Oil as capital contributions. Home Concrete then sold substantially all of its assets to a third party purchaser for $10,623,348. The taxpayers timely filed their tax returns for 1999 in April 2000. Home Concrete elected to step-up its inside basis, or the amount that the partnership tax records compute for each partner, to equal the taxpayers' outside basis. Home Concrete again adjusted its inside basis to $10,527,250.53, including the amount of short sale proceeds earlier contributed by the taxpayers. As a result Home Concrete reported a $69,125.08 gain from the sale of its assets.\n",
      "\n",
      "The IRS did not investigate until June 2003. As a result of their investigation, the IRS determined that the partnership was formed \"solely for the purposes of tax avoidance by artificially overstating basis in the partnership interests of its purported partners.\" On September 7, 2006 the IRS issued a Final Partnership Administrative Adjustment (FPAA), in which they decreased to zero the taxpayers' reported outside bases in Home Concrete. This substantially increased the taxpayers' taxable income. Plaintiff taxpayers brought action against Internal Revenue Service (IRS) seeking to recover the increase.\n",
      "\n",
      "As a general matter, the Internal Revenue Service (IRS) has three years to assess additional tax if the agency believes that the taxpayer's return has understated the amount of tax owed. That period is extended to six years, however, if the taxpayer omits from gross income an amount which is in excess of 25 percent of the amount of gross income stated in the taxpayer's return. During the trial the Treasury Department passed a regulation stating that the six-year period for assessing tax remains open for \"all taxable years… that are the subject of any case pending before any court of competent jurisdiction… in which a decision had not become final.\" The U.S. Court of Appeals for the Fourth Circuit disagreed and found in favor of the plaintiffs.\n",
      "\n",
      "--- \n",
      "\n",
      "If you have additional text or specific sentences that require cleaning, please provide them, and I can assist further!\n",
      "No content found between the specified substrings.\n",
      "Here is the cleaned text with Supreme Court decisions (post-2019) removed, while retaining all other information:\n",
      "\n",
      "---\n",
      "\n",
      "Marcel and Lucky Brand are competitors in the apparel industry, and this dispute arises over Marcel’s allegation that Lucky Brand is infringing on its “Get Lucky” trademark through its use of “Lucky” on its merchandise in violation of an injunction entered in an earlier action between the two parties.  \n",
      "In 2003, the two parties entered into a settlement agreement to resolve a trademark dispute in which Lucky Brand agreed not to use “Get Lucky” and Marcel agreed to release certain claims it might have in the future arising out of its trademarks. The two parties contest the scope of Marcel’s release of claims, with Marcel contending that it only released claims as to infringement that occurred prior to the 2003 execution of the agreement and Lucky Brand arguing that it released any future claim Marcel may have in relation to any trademark registered prior to the execution of the agreement. Further litigation ensued.  \n",
      "In litigation between the two parties over substantially the same trademark disputes, Lucky Brand argued for its interpretation of the 2003 settlement agreement. It moved to dismiss on the basis that because the marks at issue were registered prior to the settlement agreement, Marcel released any claim alleging infringement of those marks. The district court denied the motion, concluding that it was premature to determine which claims were subject to release in the 2001 agreement. However, the district court noted that Lucky Brand was “free to raise the issue . . . again after the record is more fully developed.” Lucky Brand raised the defense again in its answer and as an affirmative defense, but not again during the litigation. After a jury trial, the district court entered judgment for Marcel, declaring that Lucky Brand infringed on Marcel’s “Get Lucky” trademark and enjoining Lucky Brand from using the “Get Lucky” mark. Lucky Brand did not appeal.  \n",
      "In 2011, Marcel filed another lawsuit against Lucky Brand alleging that the latter continued to use “Lucky Brand” mark after the injunction. Lucky Brand moved for summary judgment on the basis that Marcel’s claims were precluded by res judicata in light of the final disposition of the previous action. The district court agreed, but the Second Circuit reversed, finding the allegedly barred claims “could not possibly have been sued upon in the previous case.” On remand, Marcel filed a second amended complaint, which Lucky Brand moved to dismiss on the sole basis that the 2001 agreement barred Marcel’s claims. The district court granted the motion and rejected Marcel’s argument that Lucky Brand was precluded from raising those claims.  \n",
      "The Second Circuit vacated, concluding that the doctrine of claim preclusion (or more precisely, defense preclusion) applied in situations as this one and that it barred Lucky Brand from invoking its release defense again in this action.  \n",
      "\n",
      "--- \n",
      "\n",
      "No Supreme Court decisions (post-2019) were present in the original text, so no further modifications were necessary. All other decisions and verdicts have been retained.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "llm_reas_judge = []\n",
    "\n",
    "for f,t in zip(facts,terms):\n",
    "    persona = f\"\"\"\"\n",
    "        Your task is to clean the text that proceeds the next appearing ':' character. \n",
    "        To clean it, you must remove all information that indicates the Supreme Court's decision. \n",
    "        Please note that there are other decisions and verdicts mentioned in the text, which should \n",
    "        be retained. It is only the Supreme Court decisions which should be ommitted. Additionally, \n",
    "        any Supreme Court decisions that were made prior to the year {t} (if specified), should be retained, not removed.\n",
    "        This may require splitting a sentence and only removing the judgement part of the \n",
    "        sentence, whilst still retaining all other information in the sentence.\n",
    "        The text to clean is:\n",
    "\n",
    "    \"\"\"\n",
    "    content = persona + f\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    matches = re.search(r'\\n\\n---\\n\\n(.*?)\\n\\n---\\n\\n', str(response.choices[0].message.content), re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        result = matches.group(1)\n",
    "        llm_reas_judge.append(result)\n",
    "    else:\n",
    "        print(\"No content found between the specified substrings.\")\n",
    "        print(response.choices[0].message.content)\n",
    "        llm_reas_judge.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions[\"deepseek_cleaned\"] = llm_reas_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions.to_csv(\"deepseek_cleaned.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_v2 = pd.read_csv('cleaned_facts_winner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_v2.index = facts_v2['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions['facts_cleaned'] = facts_v2['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODFIRE_API_KEY = os.getenv(\"goodfire_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = goodfire.Client(api_key=GOODFIRE_API_KEY)\n",
    "\n",
    "# Instantiate a model variant. \n",
    "variant = goodfire.Variant(\"meta-llama/Llama-3.3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = majority_opinions['term']\n",
    "first_party = majority_opinions['first_party']\n",
    "second_party = majority_opinions['second_party']\n",
    "facts = majority_opinions['facts_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_one_pass = []\n",
    "for y, f, s, fa in zip(year, first_party, second_party, facts):\n",
    "    response = \"\"  # Initialize an empty string to collect response\n",
    "    \n",
    "    for token in client.chat.completions.create(\n",
    "        [{\"role\": \"user\", \"content\": \n",
    "        f'''\n",
    "        You are a supreme court justice delivering a decision on a case. \n",
    "        \n",
    "        I will give you the year of the case, the first party, second party and facts of the case. \n",
    "        \n",
    "        Please determine whether you will rule in favor of the first party.\n",
    "        \n",
    "        Return an answer of TRUE if you decide in favor of the first party, or FALSE if not. \n",
    "        \n",
    "        FALSE is a catch-all response for any scenario other than a ruling in favor of the first party.\n",
    "\n",
    "        In your response, please give your answer (either TRUE or FALSE) as the first part of a response, then\n",
    "        a semicolon \";\", and then follow with your reasoning for why you gave your answer.\n",
    "\n",
    "        In your reasoning, do not use any information or examples from after the year of the case. \n",
    "         \n",
    "        This is the case:\n",
    "         \n",
    "        Year: {y}\n",
    "        First party: {f}\n",
    "        Second Party: {s}\n",
    "        Facts: {fa}\n",
    "        '''\n",
    "        }],\n",
    "        model=variant,\n",
    "        stream=True,\n",
    "    ):\n",
    "        response += token.choices[0].delta.content  # Append tokens to a string\n",
    "    \n",
    "    llama_one_pass.append(response)  # Append the full response instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_multi_pass = []\n",
    "\n",
    "for y, f, s, fa in zip(year, first_party, second_party, facts):\n",
    "    case_pass = []\n",
    "\n",
    "    for iteration in range(20):\n",
    "        response = \"\"\n",
    "        \n",
    "        for token in client.chat.completions.create(\n",
    "            [{\"role\": \"user\", \"content\": f\"\"\"\n",
    "            \n",
    "            You are a supreme court justice delivering a decision on a case. \n",
    "        \n",
    "            I will give you the year of the case, the first party, second party and facts of the case. \n",
    "            \n",
    "            Please determine whether you will rule in favor of the first party.\n",
    "            \n",
    "            Return an answer of TRUE if you decide in favor of the first party, or FALSE if not. \n",
    "            \n",
    "            FALSE is a catch-all response for any scenario other than a ruling in favor of the first party.\n",
    "\n",
    "            In your response, please give your answer (either TRUE or FALSE) as the first part of a response, then\n",
    "            a semicolon \";\", and then follow with your reasoning for why you gave your answer.\n",
    "\n",
    "            In your reasoning, do not use any information or examples from after the year of the case. \n",
    "             \n",
    "            This is the case:\n",
    "            Year: {y}\n",
    "            First party: {f}\n",
    "            Second Party: {s}\n",
    "            Facts: {fa}\n",
    "            \"\"\"}],\n",
    "            model=variant,\n",
    "            stream=True,\n",
    "        ):\n",
    "            response += token.choices[0].delta.content\n",
    "\n",
    "        case_pass.append(response)\n",
    "\n",
    "    llama_multi_pass.append(case_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_reasonings = []\n",
    "for i in llama_one_pass:\n",
    "    if \";\" in i:\n",
    "        llama_reasonings.append(i.split(\";\")[1])\n",
    "    else:\n",
    "        llama_reasonings.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_predictions = []\n",
    "\n",
    "for i in llama_one_pass:\n",
    "    llama_predictions.append(i.split(\";\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_predictions_binary = []\n",
    "for i in llama_predictions:\n",
    "    if i == \"TRUE\":\n",
    "        llama_predictions.append(1)\n",
    "    else:\n",
    "        llama_predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "True     28\n",
       "False    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_opinions[\"first_party_winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_party_winner_binary = []\n",
    "\n",
    "for i in majority_opinions[\"first_party_winner\"].tolist():\n",
    "    if i == True:\n",
    "        first_party_winner_binary.append(1)\n",
    "    else:\n",
    "        first_party_winner_binary.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions[\"first_party_winner_binary\"] = first_party_winner_binary\n",
    "majority_opinions[\"llama_prediction\"] = llama_predictions_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner_binary\n",
       "1    28\n",
       "0    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_opinions[\"first_party_winner_binary\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_party_winner_binary   0   1\n",
      "llm_winner_prediction            \n",
      "0                          10  19\n",
      "1                           2   9\n"
     ]
    }
   ],
   "source": [
    "contingency_matrix = pd.crosstab(majority_opinions[\"llama_prediction\"], majority_opinions[\"first_party_winner_binary\"])\n",
    "\n",
    "print(contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions[\"llama_reasoning\"] = llama_reasonings\n",
    "majority_opinions.to_csv(\"single_pass_predictions_and_reasonings.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pass_answers = []\n",
    "multi_pass_reasonings = []\n",
    "\n",
    "for i in llama_multi_pass:\n",
    "    case_answers = []\n",
    "    case_reasonings = []\n",
    "    for j in i:\n",
    "        if \";\" in j:\n",
    "            case_answers.append(j.split(\";\")[0])\n",
    "            case_reasonings.append(j.split(\";\")[1])\n",
    "        else:\n",
    "            case_answers.append(\"FALSE\")\n",
    "            case_reasonings.append(j)\n",
    "    multi_pass_answers.append(case_answers)\n",
    "    multi_pass_reasonings.append(case_reasonings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n"
     ]
    }
   ],
   "source": [
    "consistency_scores = []\n",
    "\n",
    "for i in multi_pass_answers:\n",
    "    counter = Counter(i)\n",
    "    consistency_scores.append((counter.most_common(1)[0][1]/20)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions[\"llama_consistency_scores\"] = consistency_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_opinions.to_csv(\"llama_consistency_scores.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "multi_pass_deepseek = []\n",
    "\n",
    "for y, f, s, fa in zip(year, first_party, second_party, facts):\n",
    "    case_pass = []\n",
    "\n",
    "    for iteration in range(20):\n",
    "\n",
    "        persona = f\"\"\"\"\n",
    "            You are a Supreme Court Justice delivering a decision on a case. \n",
    "        \n",
    "            I will give you the year of the case, the first party, second party and facts of the case. \n",
    "            \n",
    "            Please determine whether you will rule in favor of the first party.\n",
    "            \n",
    "            Return an answer of TRUE if you decide in favor of the first party, or FALSE if not. \n",
    "            \n",
    "            FALSE is a catch-all response for any scenario other than a ruling in favor of the first party.\n",
    "\n",
    "            In your response, please give your answer (either TRUE or FALSE) as the first part of a response, then\n",
    "            a semicolon \";\", and then follow with your reasoning for why you gave your answer.\n",
    "\n",
    "            In your reasoning, do not use any information or examples from after the year of the case. \n",
    "             \n",
    "            This is the case:\n",
    "            Year: {y}\n",
    "            First party: {f}\n",
    "            Second Party: {s}\n",
    "            Facts: {fa}\n",
    "\n",
    "        \"\"\"\n",
    "        content = persona + f\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a Supreme Court Justice.\"},\n",
    "                {\"role\": \"user\", \"content\": content},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        case_pass.append(response)\n",
    "\n",
    "    multi_pass_deepseek.append(case_pass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
